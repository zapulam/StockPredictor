{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zpulliam\\Anaconda3\\envs\\ds\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tst import TimeSeriesTransformer\n",
    "\n",
    "## Model parameters\n",
    "dim_val = 512 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 8 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_decoder_layers = 4 # Number of times the decoder layer is stacked in the decoder\n",
    "n_encoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 1 # The number of input variables. 1 if univariate forecasting.\n",
    "dec_seq_len = 92 # length of input given to decoder. Can have any integer value.\n",
    "enc_seq_len = 153 # length of input given to encoder. Can have any integer value.\n",
    "output_sequence_length = 58 # Length of the target sequence, i.e. how many time steps should your forecast cover\n",
    "max_seq_len = enc_seq_len # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "    dim_val=dim_val,\n",
    "    batch_first=True,\n",
    "    input_size=input_size, \n",
    "    dec_seq_len=dec_seq_len,\n",
    "    out_seq_len=output_sequence_length, \n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_heads=n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SP_500(Dataset):\n",
    "    def __init__(self, frequency, splits):\n",
    "        # frequency = daily, weekly, or monthly (from 'frequency'_prices folder)\n",
    "        # splits = number of times a single cvs file will be split\n",
    "\n",
    "        self.frequency = frequency\n",
    "        self.folder = frequency + '_prices'\n",
    "        self.splits = splits\n",
    "\n",
    "        self.files = os.listdir(self.folder)\n",
    "        new_files = []\n",
    "\n",
    "        # set max file length ( 5 years of data )\n",
    "        max = 0\n",
    "        for file in self.files:\n",
    "            data = pd.read_csv(os.path.join(self.folder, file), index_col=0)\n",
    "            if len(data.index) > max:\n",
    "                max = len(data.index)\n",
    "\n",
    "        # remove files with less than 5 years of data\n",
    "        for file in self.files:\n",
    "            data = pd.read_csv(os.path.join(self.folder, file), index_col=0)\n",
    "            if len(data.index) == max:\n",
    "                new_files.append(file)\n",
    "\n",
    "        self.files = new_files\n",
    "        self.data = []   # stores lists of csv and partition: [AAPL.csv, n]\n",
    "\n",
    "        # create partitions list: [AAPL.csv, n]\n",
    "        for file in self.files:\n",
    "            for i in range(self.splits):\n",
    "                self.data.append([file, i])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.data[idx][0]        # cvs to read from\n",
    "        partition = self.data[idx][1]   # partion based on number of splits\n",
    "\n",
    "        data = pd.read_csv(os.path.join(self.folder, file), index_col=0)\n",
    "        data_split = np.array_split(data, self.splits)\n",
    "\n",
    "        # ensure all splits are of equal length\n",
    "        if data_split[0].shape[0] != data_split[-1].shape[0]:\n",
    "            for i, sub in enumerate(data_split):\n",
    "                if sub.shape[0] != data_split[-1].shape[0]:\n",
    "                    data_split[i] = data_split[i][0:-1]\n",
    "\n",
    "        data = data_split[partition]\n",
    "\n",
    "        x = data[['Open', 'High', 'Low', 'Volume', 'Close']]    # data\n",
    "\n",
    "        mins, maxs = x.min(), x.max()                           # values for normalization\n",
    "\n",
    "        x = (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "        x = x.to_numpy()\n",
    "        mins = mins.to_numpy()\n",
    "        maxs = maxs.to_numpy()\n",
    "\n",
    "        return x, mins, maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 torch.Size([2, 100, 5]) torch.Size([2, 100, 1])\n",
      "213 torch.Size([2, 100, 5]) torch.Size([2, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from rnn import LSTM, GRU\n",
    "\n",
    "model = LSTM(input_dim=5, hidden_dim=32, output_dim=1, num_layers=2)\n",
    "\n",
    "dataset = SP_500(4)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "lookback = 100\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    #print(data[0].shape)\n",
    "\n",
    "    seqs = []\n",
    "\n",
    "    for i in range(data[0].shape[1]-1 - lookback): \n",
    "        seqs.append([data[0][:,i: i + lookback, :], data[1][:, i+1: i+1 + lookback, :]])\n",
    "\n",
    "    print(len(seqs), seqs[0][0].shape, seqs[0][1].shape) # num sequences, train, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(2, 5, 3)\n",
    "y = torch.rand(2, 5, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class poop():\n",
    "    def __init__(self, x, y):\n",
    "        self.z = 34\n",
    "        self.a = x\n",
    "        self.b = y\n",
    "    \n",
    "    def otherfunction():\n",
    "        return 0\n",
    "\n",
    "    def addition(self, i, j):\n",
    "        k = self.a + self.b\n",
    "        return k\n",
    "\n",
    "first_class = poop(2, 3)\n",
    "\n",
    "first_class.addition(9, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffd344d186918f69b2eea9b2c168952deeb2cf1752eb38755dce2e85e3758b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
